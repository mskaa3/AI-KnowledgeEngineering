{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5de0b2e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  1000 non-null   object\n",
      " 1   Result  1000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "opinions=pd.read_csv(\"amazon.txt\",sep='\\t', header=None, names=[\"Review\",\"Result\"],quoting=3)\n",
    "opinions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e53cb89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    So there is no way for me to plug it in here i...\n",
       "1                          Good case, Excellent value.\n",
       "2                               Great for the jawbone.\n",
       "3    Tied to charger for conversations lasting more...\n",
       "4                                    The mic is great.\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions['Review'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da72d0",
   "metadata": {},
   "source": [
    "Firs I cleaned the data. I got rid of the stopwords(meaningless words such as a, an etc) and I stemmed words with the same origin (then they are treated as same word). Moreover, all elements which are not words, such as numbers, punctuation etc were replaced with a whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ba0015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              so way plug us unless i go convert\n",
       "1                            good case excel valu\n",
       "2                                    great jawbon\n",
       "3    tie charger convers last minut major problem\n",
       "4                                   the mic great\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import re\n",
    "# connecting different instances of the word to one, like connection, connections, connective, connected\n",
    "stemmer=SnowballStemmer('english')\n",
    "# stop words are meaningless common words like and, a, an\n",
    "stop_words=stopwords.words(\"english\")\n",
    "# applying lambda to each column: replacing every chaaracter that is not a letter to a blank, splitting into words,\n",
    "# and we use stemmer if the word is not a stopword, and than we join it together into a sentence with blank\n",
    "opinions[\"cleaned\"]=opinions[\"Review\"].apply(lambda x: \" \".join(\n",
    "    [stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\",\" \",x).split() if i not in stop_words]).lower())\n",
    "opinions[\"cleaned\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23c6c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(opinions['Review'],opinions[\"Result\"],test_size=0.2)\n",
    "pip=Pipeline([('vectorizer',TfidfVectorizer(ngram_range=(1, 2),stop_words=\"english\")),\n",
    "              ('classifier',LinearSVC(random_state=10))\n",
    "             ])\n",
    "opinions_model=pip.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83904de",
   "metadata": {},
   "source": [
    "To perform machine learning on text, we need to present is as a numerical vector, which I obtain with vectorizer. Than I apply Linear SVC model to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e09657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7625 , 0.775  , 0.80625, 0.7625 , 0.775  ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "y_train_pred=cross_val_score(opinions_model,x_train,y_train,cv=5)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d538d9",
   "metadata": {},
   "source": [
    "Since cross value by itself does not say much about details of our model successes/faults, we can present results as a confusion matrix, which shows precisely number of properly classified values as well as true values classified as false and reverse. The ideal result would be diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3403d0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[308,  94],\n",
       "       [ 85, 313]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_train_pred=cross_val_predict(opinions_model,x_train,y_train,cv=5)\n",
    "confusion_matrix(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93f17f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769041769041769\n",
      "0.7864321608040201\n",
      "0.77625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score\n",
    "print(precision_score(y_train,y_train_pred))\n",
    "print(recall_score(y_train,y_train_pred))\n",
    "print(accuracy_score(y_train,y_train_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c3d86",
   "metadata": {},
   "source": [
    "Precision,recall and accuracy tells us more about ratios between true positives and all positives, all true and false positives and general ratio between properly classfied data and all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e514d4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_model.predict([\"My daughter was really satisfied with her gift\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "031af0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinions_model.predict([\"My daughter totally hated her gift, I will never buy it again\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c6ec6",
   "metadata": {},
   "source": [
    "Just as a test, two similar sentences but with different strongly positively/negatively characterized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a95cd07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.775, 0.8  , 0.625, 0.675, 0.85 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred=cross_val_score(opinions_model,x_test,y_test,cv=5)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "72fa7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72, 26],\n",
       "       [25, 77]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred=cross_val_predict(opinions_model,x_test,y_test,cv=5)\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "69f70223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475728155339806\n",
      "0.7549019607843137\n",
      "0.745\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_test,y_test_pred))\n",
    "print(recall_score(y_test,y_test_pred))\n",
    "print(accuracy_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf34fb",
   "metadata": {},
   "source": [
    "Similar experiments on the accuracy performed on the test set. As we see, results are slightly worse, but not much different, so I consider it as a good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36804ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.785"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_pip=Pipeline([('vectorizer',TfidfVectorizer(ngram_range=(1, 2),stop_words=\"english\")),\n",
    "              ('classifier',LinearSVC(penalty='l1',dual=False))\n",
    "             ])\n",
    "second_opinions_model=second_pip.fit(x_train,y_train)\n",
    "second_opinions_model.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f12034",
   "metadata": {},
   "source": [
    "Tests on the same model, but woth penalty changed to L1. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ae0481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.775, 0.65 , 0.575, 0.75 , 0.775])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_second=cross_val_score(second_opinions_model,x_test,y_test,cv=5)\n",
    "y_test_pred_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62b72cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[84, 20],\n",
       "       [39, 57]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_second=cross_val_predict(second_opinions_model,x_test,y_test,cv=5)\n",
    "confusion_matrix(y_test,y_test_pred_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa2658d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7402597402597403\n",
      "0.59375\n",
      "0.705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(precision_score(y_test,y_test_pred_second))\n",
    "print(recall_score(y_test,y_test_pred_second))\n",
    "print(accuracy_score(y_test,y_test_pred_second))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fde3a818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "pip_gradient=Pipeline([('vectorizer',TfidfVectorizer(ngram_range=(1, 2),stop_words=\"english\",max_df=8)),\n",
    "              ('classifier', GradientBoostingClassifier(random_state=20))\n",
    "             ])\n",
    "opinions_model_gradient=pip_gradient.fit(x_train,y_train)\n",
    "opinions_model_gradient.score(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb317b5",
   "metadata": {},
   "source": [
    "Finally, same tests performed on totally different model. As we see, in general I would say the best fit would be the first one, because it's results are much better and consistent than the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7508411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7  , 0.675, 0.625, 0.7  , 0.725])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_gradient=cross_val_score(opinions_model_gradient,x_test,y_test,cv=5)\n",
    "y_train_pred_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c68ab32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94, 10],\n",
       "       [53, 43]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred_gradient=cross_val_predict(opinions_model_gradient,x_test,y_test,cv=5)\n",
    "confusion_matrix(y_test,y_test_pred_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04935ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113207547169812\n",
      "0.4479166666666667\n",
      "0.685\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(y_test,y_test_pred_gradient))\n",
    "print(recall_score(y_test,y_test_pred_gradient))\n",
    "print(accuracy_score(y_test,y_test_pred_gradient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b059ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f846fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
